# ğŸ”¬ RAG Project v1: Innovatech Solutions AI Assistant

<div align="center">

![Python](https://img.shields.io/badge/python-3.13-blue.svg)
![LangChain](https://img.shields.io/badge/LangChain-1.0-green.svg)
![ChromaDB](https://img.shields.io/badge/ChromaDB-1.3-orange.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

**My Retrieval-Augmented Generation (RAG) system v1 for intelligent enterprise knowledge management**

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Technology Stack](#-technology-stack)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Usage](#-usage)
- [Architecture](#-architecture)
- [Knowledge Base](#-knowledge-base)
- [Configuration](#-configuration)
- [Development](#-development)
- [Troubleshooting](#-troubleshooting)
- [License](#-license)

---

## ğŸŒŸ Overview

**RAG Project v1** is a production-ready Retrieval-Augmented Generation system built for **Innovatech Solutions**, demonstrating how AI can transform enterprise knowledge management. The system enables natural language queries over organizational data including company information, products, employees, and contracts.


## âœ¨ Features



---

## ğŸ› ï¸ Technology Stack

### Core Technologies

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Language** | Python 3.13 | Main programming language |
| **LLM Framework** | LangChain v1.0 | Orchestration and RAG pipeline |
| **Vector Database** | ChromaDB | Embeddings storage and retrieval |
| **Embeddings** | HuggingFace (all-MiniLM-L6-v2) | Document and query embeddings |
| **LLM** | OpenAI / Ollama (Gemma3 4B) | Response generation |
| **UI Framework** | Gradio | Web interface |
| **Text Processing** | LangChain Text Splitters | Document chunking |




## ğŸ“ Project Structure

```
RAG-Project-v1/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ pyproject.toml                     # Project configuration (uv)
â”œâ”€â”€ uv.lock                            # Dependency lock file
â”œâ”€â”€ .gitignore                         # Git ignore rules
â”œâ”€â”€ .env                               # Environment variables (create this)
â”‚
â””â”€â”€ root-project/                      # Main application folder
    â”œâ”€â”€ app.py                         # ğŸš€ Main Gradio web application (START HERE)
    â”‚
    â”œâ”€â”€ implementation/                # Core RAG implementation
    â”‚   â”œâ”€â”€ ingest.py                  # Document ingestion & embedding creation
    â”‚   â””â”€â”€ answer.py                  # RAG pipeline & question answering
    â”‚
    â”œâ”€â”€ knowledge_base/                # ğŸ“š Source documents (31 .md files)
    â”‚   â”œâ”€â”€ company/                   # Company information (4 docs)
    â”‚   â”‚   â”œâ”€â”€ about.md
    â”‚   â”‚   â”œâ”€â”€ careers.md
    â”‚   â”‚   â”œâ”€â”€ culture.md
    â”‚   â”‚   â””â”€â”€ overview.md
    â”‚   â”‚
    â”‚   â”œâ”€â”€ products/                  # Product documentation (5 docs)
    â”‚   â”‚   â”œâ”€â”€ ClarityLens.md
    â”‚   â”‚   â”œâ”€â”€ Continuum.md
    â”‚   â”‚   â”œâ”€â”€ EchoSphere.md
    â”‚   â”‚   â”œâ”€â”€ Guardian.md
    â”‚   â”‚   â””â”€â”€ SynapseEngine.md
    â”‚   â”‚
    â”‚   â”œâ”€â”€ employees/                 # Employee profiles (10 docs)
    â”‚   â”‚   â”œâ”€â”€ Gideon Cross.md
    â”‚   â”‚   â”œâ”€â”€ Isolde Beaumont.md
    â”‚   â”‚   â”œâ”€â”€ Jia Li.md
    â”‚   â”‚   â”œâ”€â”€ Kaelen Vance.md
    â”‚   â”‚   â”œâ”€â”€ Linnea Vega.md
    â”‚   â”‚   â”œâ”€â”€ Orion Fletcher.md
    â”‚   â”‚   â”œâ”€â”€ Ronan Dexter.md
    â”‚   â”‚   â”œâ”€â”€ Seraphina Jones.md
    â”‚   â”‚   â”œâ”€â”€ Silas Thorne.md
    â”‚   â”‚   â””â”€â”€ Tessa McRae.md
    â”‚   â”‚
    â”‚   â””â”€â”€ contracts/                 # Business contracts (12 docs)
    â”‚       â”œâ”€â”€ API Data License with Veridian Datafeeds for InsightIQ.md
    â”‚       â”œâ”€â”€ Cloud Services Agreement with Stratus Infrastructure for Hosting.md
    â”‚       â”œâ”€â”€ Co-Marketing Agreement with Orion CRM for SynapseChat Integration.md
    â”‚       â”œâ”€â”€ Consulting Agreement with Navigator Strategy for APAC GTM.md
    â”‚       â”œâ”€â”€ Data Processing Addendum with Zenith Financial Group for InsightIQ.md
    â”‚       â”œâ”€â”€ Hybrid MSA with Fusion Robotics for NexusFlow and CoreLLM.md
    â”‚       â”œâ”€â”€ Master Service Agreement with Momentum Machines for NexusFlow.md
    â”‚       â”œâ”€â”€ Master Service Agreement with Silverline Logistics for NexusFlow.md
    â”‚       â”œâ”€â”€ OEM Partnership Agreement with Quantum Leap AI for CoreLLM API.md
    â”‚       â”œâ”€â”€ Office Lease Agreement with Cityscape Realty for SF Headquarters.md
    â”‚       â”œâ”€â”€ Reseller Agreement with Catalyst Partners for EMEA Region.md
    â”‚       â””â”€â”€ Statement of Work with Cygnus Security for Penetration Test.md
    â”‚
    â”œâ”€â”€ experiments/                   # Jupyter notebooks for testing
    â”‚   â””â”€â”€ experiment.ipynb           # RAG experimentation notebook
    â”‚
    â””â”€â”€ vector_db/                     # âš ï¸ Auto-generated (gitignored)
        â””â”€â”€ [Created locally when you run ingest.py]

Note: vector_db/ folders are auto-generated and not tracked in Git
```

---

## ğŸš€ Installation

### Prerequisites

- **Git** (for cloning the repository)
- **OpenAI API Key** OR **Ollama** installed locally (for local LLMs)

### Step 1: Clone the Repository

```bash
git clone https://github.com/FamilOrujov/RAG-Project-v1.git
cd RAG-Project-v1
```

### Step 2: Create Virtual Environment

#### Using venv (Standard Python)
```bash
python -m venv .venv

# On Windows
.venv\Scripts\activate

# On macOS/Linux
source .venv/bin/activate
```

#### Using uv (Faster Alternative)
```bash
uv venv
uv pip install -r requirements.txt
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Set Up Environment Variables

Create a `.env` file in the project root:

```bash
# For OpenAI API
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Other API keys
ANTHROPIC_API_KEY=your_anthropic_key
GOOGLE_API_KEY=your_google_key
```

### Step 5: Download Ollama (Optional - For Local LLMs)

If you want to use local models instead of OpenAI:

1. Download and install [Ollama](https://ollama.ai/)
2. Pull a model:
```bash
ollama pull gemma2:2b
# or
ollama pull llama3.1:8b
```

---

## ğŸ¯ Usage

### Quick Start

#### 1. Ingest Documents (First Time Setup)

This step processes all documents in the `knowledge_base/` folder, creates embeddings, and stores them in ChromaDB.

```bash
cd root-project
python implementation/ingest.py
```

**Expected Output:**
```
There are 156 vectors with 384 dimensions in the vector store
Ingestion complete
```

#### 2. Launch the Web Interface

```bash
python app.py
```

The Gradio interface will open in your browser at `http://localhost:7860`

### Using the Application

1. **Ask Questions**: Type your question in the text box
2. **View Responses**: The AI assistant will provide an answer
3. **Check Sources**: The right panel shows which documents were used
4. **Continue Conversation**: Ask follow-up questions for context-aware responses

### Example Queries

```
â“ What products does Innovatech Solutions offer?
â“ Tell me about the company's mission and vision
â“ Who is Seraphina Jones and what does she do?
â“ What is the NexusFlow platform?
â“ Tell me about the contract with Momentum Machines
â“ What are the key features of the Synapse Engine?
```

---

## ğŸ—ï¸ Architecture

### RAG Pipeline Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG PIPELINE                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. INGESTION PHASE (One-time / When knowledge base updates)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Load Docs   â”‚ â”€â”€â”€> â”‚ Split Chunks â”‚ â”€â”€â”€> â”‚   Embed &    â”‚
   â”‚  from KB     â”‚      â”‚  (500 chars) â”‚      â”‚   Store in   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   ChromaDB   â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. QUERY PHASE (Real-time)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ User Query   â”‚ â”€â”€â”€> â”‚   Embed &    â”‚ â”€â”€â”€> â”‚   Retrieve   â”‚
   â”‚              â”‚      â”‚ Vector Searchâ”‚      â”‚  Top K Docs  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       v
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Return     â”‚ <â”€â”€â”€ â”‚     LLM      â”‚ <â”€â”€â”€ â”‚   Format     â”‚
   â”‚   Answer     â”‚      â”‚   Generate   â”‚      â”‚   Prompt     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Details

#### 1. Document Ingestion (`ingest.py`)

**Purpose**: Process raw documents and create searchable embeddings

**Steps**:
1. **Load Documents**: Scans `knowledge_base/` for all `.md` files
2. **Add Metadata**: Tags documents with type (company, products, employees, contracts)
3. **Text Splitting**: 
   - Chunk size: 500 characters
   - Overlap: 200 characters (preserves context)
4. **Embedding Creation**: Uses `all-MiniLM-L6-v2` (384 dimensions)
5. **Vector Storage**: Stores in ChromaDB at `root-project/vector_db/`

**Key Functions**:
- `fetch_documents()`: Loads all markdown files with metadata
- `create_chunks()`: Splits documents into overlapping chunks
- `create_embeddings()`: Creates and stores vector embeddings

#### 2. Question Answering (`answer.py`)

**Purpose**: Handle user queries and generate responses using RAG

**Steps**:
1. **Query Processing**: Combines current question with conversation history
2. **Retrieval**: Fetches top K=10 most relevant document chunks
3. **Context Formatting**: Assembles retrieved documents into context
4. **Prompt Construction**: Injects context into system prompt
5. **LLM Generation**: Generates response using chat history
6. **Response Return**: Returns answer + source documents

**Key Functions**:
- `fetch_context()`: Retrieves relevant documents via semantic search
- `combined_question()`: Merges conversation history for better retrieval
- `answer_question()`: Main RAG pipeline orchestration

#### 3. Web Interface (`app.py`)

**Purpose**: User-friendly Gradio interface for interaction

**Features**:
- Two-column layout: Chat on left, sources on right
- Real-time streaming responses
- Copy button for answers
- Formatted source attribution with highlighting
- Conversation history management

---

## ğŸ“š Knowledge Base

### Structure

Inside the knowledge_base folder, you'll see 4 folders: company, contracts, employees, and products. All the .md files inside are **synthetic data**, which I created using Frontier LLMs (ChatGPT 5 and Gemini 2.5 pro).

The knowledge base contains **31 markdown documents** across 4 categories:

| Category | Count | Description |
|----------|-------|-------------|
| **Company** | 4 docs | Mission, culture, history, careers |
| **Products** | 5 docs | SynapseEngine, ClarityLens, Continuum, EchoSphere, Guardian |
| **Employees** | 10 docs | Employee profiles with roles and responsibilities |
| **Contracts** | 12 docs | Business agreements, MSAs, partnerships |



## âš™ï¸ Configuration

### Key Parameters

Edit these in `answer.py` and `ingest.py`:

#### Retrieval Settings (`answer.py`)

```python
RETRIEVAL_K = 10  # Number of document chunks to retrieve
                  # Higher = more context, slower responses
                  # Lower = faster, potentially less comprehensive
```

#### Chunking Settings (`ingest.py`)

```python
chunk_size = 500       # Characters per chunk
chunk_overlap = 200    # Overlap between chunks (preserves context)
```

#### Model Selection (`answer.py`)

**Option 1: OpenAI API**
```python
llm = ChatOpenAI(temperature=0, model_name="gpt-4o-mini")
```

**Option 2: Local LLM (Ollama)**
```python
llm = ChatOpenAI(
    temperature=0.7, 
    model_name='gemma2:2b',  # or llama3.1:8b
    base_url='http://localhost:11434/v1', 
    api_key='ollama'
)
```

#### Embedding Model (`ingest.py` and `answer.py`)

```python
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
# Alternatives:
# - all-mpnet-base-v2 (higher quality, slower)
# - all-MiniLM-L12-v2 (balanced)
```

---

## ğŸ”§ Development

### Project Structure for Developers

- **`implementation/ingest.py`**: Modify document loading, chunking strategy, embedding model
- **`implementation/answer.py`**: Customize RAG pipeline, prompts, retrieval strategy
- **`app.py`**: Change UI, add features, modify chat interface
- **`experiments/`**: Jupyter notebooks for testing and experimentation

### Running Experiments

The `experiments/` folder contains Jupyter notebooks for testing:

```bash
cd root-project/experiments
jupyter notebook experiment.ipynb
```

### Testing Different Configurations

1. **Test Chunk Sizes**: Modify `chunk_size` in `ingest.py` and re-ingest
2. **Test Retrieval K**: Adjust `RETRIEVAL_K` in `answer.py` and restart app
3. **Test Different LLMs**: Change model in `answer.py`
4. **Test Different Embeddings**: Change embedding model and re-ingest

### Debugging

Enable verbose output by adding to `answer.py`:

```python
import logging
logging.basicConfig(level=logging.DEBUG)



